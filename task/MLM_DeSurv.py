import sys
import os
sys.path.insert(0, '../')

import pytorch_pretrained_bert as Bert
import pandas as pd
import pickle as pkl
import argparse
import sklearn.metrics as skm
import numpy as np
import torch
import time
import torch.nn as nn
from torch.utils.data import DataLoader

from common.common import load_obj
from common.common import create_folder
from common.pytorch import load_model
from model.utils import age_vocab
from model.optimiser import adam
from model.MLM import BertForMaskedLM
from dataLoader.Survival import Survival
from MLM import BertConfig, TrainConfig

from DeSurv.src.classes import ODESurvSingle, ODESurvMultiple


def main():

	N_EPOCHS = 50
    
	global_params = {
        'max_seq_len': 64,
	    'max_age': 110,
	    'month': 1,
	    'age_symbol': None,
	    'min_visit': 2,
	    'gradient_accumulation_steps': 1
    }

	optim_param = {
	    'lr': 3e-5,
	    'warmup_proportion': 0.1,
	    'weight_decay': 0.01
	}

	train_params = {
	    'batch_size': 256,
	    'use_cuda': True,
	    'max_len_seq': global_params['max_seq_len'],
	    'device': torch.device('cuda' if torch.cuda.is_available() and train_params['use_cuda'] else 'cpu')
	}

	parser = argparse.ArgumentParser()
	parser.add_argument("--train_parquet", type=str, 
					 	default="task/example_data/example_data.parquet",
                        help="Path to BEHRT-adapted parquet with columns: patid, caliber_id(list[str]), age(list[int]), survival_label(tuple[str,float]).")
	parser.add_argument("--token2idx_pkl", type=str, 
					 	default="task/example_data/token2idx",
                        help="Path to token2idx pkl generated by your BEHRT pipeline.")
	parser.add_argument("--behrt_ckpt", type=str, 
					 	default="task/example_data/MLM.ckpt",
                        help="Path to saved BEHRT MLM weights (state_dict) to use for feature extraction.")
	parser.add_argument("--event_code", type=list, 
					 	default=['I10', 'E78'],
                        help="Event code for single-risk target (present -> event=1, else censored).")
	parser.add_argument("--competing_risks",
					 	default=False,
						action="store_true", 
						help="Use ODESurvMultiple (requires cause indices).")
	args = parser.parse_args()

	model_path = "/".join(args.behrt_ckpt.split('/')[:-1])
	model_name = args.behrt_ckpt.split('/')[-1]

	file_config = {
        'data': args.train_parquet,
		'vocab': args.token2idx_pkl,
	    'model_path': model_path, # where to save model
	    'model_name': model_name, # model name
	    'file_name': 'MLM_DeSurv_log.out',  # log path
	}
	create_folder(file_config['model_path'])

	# ---------- Vocab / age vocab ----------
	BertVocab = load_obj(file_config['vocab'])
	# print(BertVocab)
	ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])
	# print(ageVocab)

    # ---------- Load EHR ----------
	data = pd.read_parquet(args.train_parquet)
	# remove patients with visits less than min visit
	data['length'] = data['caliber_id'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))
	data = data[data['length'] >= global_params['min_visit']]
	data = data.reset_index(drop=True)
	data["patid"] = data.index
	# print(data)

	# ---------- make dataset and dataloader ----------

	if args.competing_risks:
		# for competing risks, convert event codes to one-hot (1,2,3,..., K if in event_code, else 0)
		event_pos = {code: i + 1 for i, code in enumerate(args.event_code)}
		label2idx = {k: event_pos.get(k, 0) for k in BertVocab['token2idx']}
		print(f"Using a competing risks model with causes: {args.event_code}")
	else:
		# for single-risk, convert event codes to binary (1 if in event_code, else 0)
		label2idx = {key:1 if key in args.event_code else 0 for key, item in BertVocab['token2idx'].items()}
		print(f"Using a single-risk model with union of events: {args.event_code}")
	print(f"These are combined to outcomes with label2idx mapping: {label2idx}")

	Dset = Survival(
		BertVocab['token2idx'],
		label2idx,
		ageVocab,
		data,
		max_len=train_params['max_len_seq'],
		code='caliber_id',
		label="target_event",
		label_time="target_time",
		)
	trainload = DataLoader(
		dataset=Dset,
		batch_size=train_params['batch_size'],
		shuffle=True,
		num_workers=3
		)
	
	# ---------- Load BEHRT MLM ----------
	model_config = {
	    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding
	    'hidden_size': 288, # word embedding and seg embedding hidden size
	    'seg_vocab_size': 2, # number of vocab for seg embedding
	    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding
	    'max_position_embedding': train_params['max_len_seq'], # maximum number of tokens
	    'hidden_dropout_prob': 0.1, # dropout rate
	    'num_hidden_layers': 6, # number of multi-head attention layers required
	    'num_attention_heads': 12, # number of attention heads
	    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate
	    'intermediate_size': 512, # the size of the "intermediate" layer in the transformer encoder
	    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler "gelu", 'relu', 'swish' are supported
	    'initializer_range': 0.02, # parameter weight initializer range
	}

	conf = BertConfig(model_config)
	behrt_model = BertForMaskedLM(conf)	
	# print(behrt_model)

	sd = torch.load(args.behrt_ckpt, map_location="cpu")
	behrt_model.load_state_dict(sd, strict=False)
	behrt_model.eval()

	# ---------- Create DeSurv model ----------
	if args.competing_risks:
		print(f"Using ODESurvMultiple with causes: {args.event_code}")
		desurv_model = ODESurvMultiple(
			lr=optim_param['lr'],
			cov_dim=model_config['hidden_size'],
			hidden_dim=32,
			num_risks=len(args.event_code),
			device=train_params["device"],
			)
		
	else:
		print(f"Using ODESurvSingle for union of events: {args.event_code}")
		desurv_model = ODESurvSingle(
			lr=optim_param['lr'],
			cov_dim=model_config['hidden_size'],
			hidden_dim=32,
			device=train_params["device"],
			)
		# print(BertVocab['token2idx'])

	# print(desurv_model)

	optimizer = desurv_model.optimizer

	# if data_loader_val is not None:
	# 	best_val_loss = np.inf
	# 	wait = 0

	for epoch in range(N_EPOCHS):

		train_loss = 0.0

		for step, batch in enumerate(trainload):

			age_ids, input_ids, posi_ids, segment_ids, attMask, target_label, target_time, patient_id = batch

			# Forward pass BEHRT MLM trained model to extract features
			with torch.no_grad():
				seq_output, pooled_output = behrt_model.bert(
					input_ids,
					age_ids,
					segment_ids,
					posi_ids,
					attMask,
					output_all_encoded_layers=False,
				)
				# print(f"seq_output: {seq_output.shape}")
				# print(f"pooled_output: {pooled_output.shape}")

			# Forward DeSurv model for survival prediction using pooled_output as input features
			argsort_t = torch.argsort(target_time)
			x_ = pooled_output[argsort_t,:].to(train_params["device"]).squeeze(1)
			t_ = target_time[argsort_t].to(train_params["device"]).squeeze(1).squeeze(1)
			k_ = target_label[argsort_t].to(train_params["device"]).squeeze(1).squeeze(1)

			optimizer.zero_grad()
			loss = desurv_model.forward(x_,t_,k_)
			loss.backward()
			optimizer.step()

			train_loss += loss.item()

		if epoch % 2 == 0:
			print(f"\tEpoch: {epoch:2}. Total loss: {train_loss:11.2f}")
			# if data_loader_val is not None:
			# 	val_loss = 0
			# 	for batch_idx, (x, t, k) in enumerate(data_loader_val):
			# 		argsort_t = torch.argsort(t)
			# 		x_ = x[argsort_t,:].to(self.odenet.device)
			# 		t_ = t[argsort_t].to(self.odenet.device)
			# 		k_ = k[argsort_t].to(self.odenet.device)

			# 		loss = self.forward(x_,t_,k_)
			# 		val_loss += loss.item()

			# 	if val_loss < best_val_loss:
			# 		best_val_loss = val_loss
			# 		wait = 0
			# 		print(f"best_epoch: {epoch}")
			# 		torch.save(self.state_dict(), "low_")
			# 	else:
			# 		wait += 1

			# 	if wait > max_wait:
			# 		state_dict = torch.load("low_")
			# 		self.load_state_dict(state_dict)
			# 		return

			# 	print(f"\tEpoch: {epoch:2}. Total val loss: {val_loss:11.2f}")
	# if data_loader_val is not None:
	# 	print("loading low_")
	# 	state_dict = torch.load("low_")
	# 	self.load_state_dict(state_dict)
	# 	return


	# f = open(os.path.join(file_config['model_path'], file_config['file_name']), "w")
	# f.write('{}\t{}\t{}\n'.format('epoch', 'loss', 'time'))
	# for e in range(50):
	# 	loss, time_cost = train(e, trainload)
	# 	loss = loss/10
	# 	f.write('{}\t{}\t{}\n'.format(e, loss, time_cost))
	# f.close() 

if __name__ == "__main__":

	main()
